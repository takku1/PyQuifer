"""
Validation: PyQuifer Hierarchical Predictive Coding vs Friston Equations

Validates that HierarchicalPredictiveCoding implements the correct dynamics:

Friston's predictive coding equations:
  prediction_i = g_i(beliefs_{i+1})          # top-down prediction
  error_i = input_i - prediction_i            # prediction error
  d_beliefs_i = lr * precision_i * error_i    # update beliefs

Key properties to validate:
1. Free energy decreases over repeated presentations of same input
2. Prediction errors decrease as beliefs converge
3. Precision weighting correctly modulates error influence
4. Hierarchical message passing: errors flow up, predictions flow down

Reference: Friston (2005) "A theory of cortical responses"
"""

import torch
import numpy as np
import sys
import os

sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..', 'src'))

from pyquifer.hierarchical_predictive import PredictiveLevel, HierarchicalPredictiveCoding
from pyquifer.precision_weighting import PrecisionEstimator, PrecisionGate, AttentionAsPrecision


def test_free_energy_decreases():
    """
    Test 1: Free energy should decrease when same input is repeated.

    This is the core property of variational inference â€” the model
    minimizes surprise (free energy) by updating beliefs.
    """
    hpc = HierarchicalPredictiveCoding(
        level_dims=[32, 16, 8],
        lr=0.1,
        gen_lr=0.05,
    )

    torch.manual_seed(42)
    fixed_input = torch.randn(1, 32) * 0.5

    free_energies = []
    for _ in range(100):
        result = hpc(fixed_input)
        free_energies.append(result['free_energy'].item())

    # Free energy should decrease overall (not necessarily monotonically
    # due to stochastic generative model updates)
    early_avg = np.mean(free_energies[:10])
    late_avg = np.mean(free_energies[-10:])

    assert late_avg < early_avg, \
        f"Free energy didn't decrease: early={early_avg:.4f}, late={late_avg:.4f}"
    print(f"  Early FE={early_avg:.4f}, Late FE={late_avg:.4f}, reduction={early_avg - late_avg:.4f}")


def test_prediction_errors_decrease():
    """
    Test 2: Prediction errors at each level should decrease over time.
    """
    hpc = HierarchicalPredictiveCoding(
        level_dims=[32, 16, 8],
        lr=0.1,
        gen_lr=0.05,
    )

    torch.manual_seed(42)
    fixed_input = torch.randn(1, 32) * 0.5

    early_errors = []
    late_errors = []

    for i in range(100):
        result = hpc(fixed_input)
        error_norms = [e.norm().item() for e in result['errors']]
        if i < 10:
            early_errors.append(error_norms)
        if i >= 90:
            late_errors.append(error_norms)

    # Average error norms
    early_avg = np.mean(early_errors, axis=0)
    late_avg = np.mean(late_errors, axis=0)

    # Bottom level errors should decrease most (direct input)
    assert late_avg[0] < early_avg[0], \
        f"Bottom-level errors didn't decrease: {early_avg[0]:.4f} -> {late_avg[0]:.4f}"
    print(f"  Level 0 errors: {early_avg[0]:.4f} -> {late_avg[0]:.4f}")
    print(f"  Level 1 errors: {early_avg[1]:.4f} -> {late_avg[1]:.4f}")


def test_predictions_reconstruct_input():
    """
    Test 3: After convergence, predictions should reconstruct input.

    prediction_error = input - g(beliefs) should be small after convergence.
    Beliefs live in a learned latent space, so we check prediction accuracy,
    not belief-input correlation.
    """
    hpc = HierarchicalPredictiveCoding(
        level_dims=[16, 8],
        lr=0.15,
        gen_lr=0.05,
    )

    torch.manual_seed(42)
    fixed_input = torch.randn(1, 16)

    # Let beliefs converge
    for _ in range(200):
        result = hpc(fixed_input)

    # Bottom-level prediction error should be small
    bottom_error = result['errors'][0].norm().item()
    input_norm = fixed_input.norm().item()
    relative_error = bottom_error / (input_norm + 1e-8)

    print(f"  Bottom error norm: {bottom_error:.4f}")
    print(f"  Input norm: {input_norm:.4f}")
    print(f"  Relative error: {relative_error:.4f}")
    assert relative_error < 0.1, f"Predictions don't reconstruct input: relative error={relative_error:.4f}"


def test_precision_modulates_learning():
    """
    Test 4: External precision should not crash and affects weighted errors.

    In this implementation, beliefs update via recognition model (amortized
    inference), not directly from precision-weighted errors. Precision scales
    the weighted_error output used by downstream modules.
    """
    hpc = HierarchicalPredictiveCoding(level_dims=[16, 8], lr=0.1)

    torch.manual_seed(42)
    fixed_input = torch.randn(1, 16) * 0.5

    # Run with default precision
    for _ in range(10):
        r_default = hpc(fixed_input)

    # Run with external precision (should not crash, dims match input)
    high_precisions = [torch.ones(1, 16) * 5.0, torch.ones(1, 16) * 5.0]
    r_high = hpc(fixed_input, precisions=high_precisions)

    print(f"  Default error norm: {r_default['errors'][0].norm().item():.4f}")
    print(f"  With precision error norm: {r_high['errors'][0].norm().item():.4f}")
    print(f"  Free energy (default): {r_default['free_energy'].item():.4f}")
    print(f"  Free energy (precision): {r_high['free_energy'].item():.4f}")

    # Main validation: no crash, reasonable outputs
    assert r_high['free_energy'].item() >= 0, "Free energy should be non-negative"


def test_hierarchy_message_passing():
    """
    Test 5: Validate bidirectional message passing.

    - Errors flow UP (bottom -> top)
    - Predictions flow DOWN (top -> bottom)
    - Each level has both errors and beliefs
    """
    hpc = HierarchicalPredictiveCoding(
        level_dims=[32, 16, 8],
        lr=0.1,
    )

    torch.manual_seed(42)
    result = hpc(torch.randn(1, 32))

    # Should have errors at each level
    assert len(result['errors']) == 3, f"Expected 3 error levels, got {len(result['errors'])}"

    # Should have beliefs at each level
    assert len(result['beliefs']) == 3, f"Expected 3 belief levels, got {len(result['beliefs'])}"

    # Error dims match INPUT dims, belief dims match BELIEF dims
    # Level 0: input=32, belief=32
    # Level 1: input=32 (beliefs from level 0), belief=16
    # Level 2: input=16 (beliefs from level 1), belief=8
    expected_error_dims = [32, 32, 16]
    expected_belief_dims = [32, 16, 8]
    for i, (e, b) in enumerate(zip(result['errors'], result['beliefs'])):
        assert e.shape[-1] == expected_error_dims[i], \
            f"Level {i} error dim {e.shape[-1]} != {expected_error_dims[i]}"
        assert b.shape[-1] == expected_belief_dims[i], \
            f"Level {i} belief dim {b.shape[-1]} != {expected_belief_dims[i]}"
    print(f"  Error dims: {[e.shape[-1] for e in result['errors']]}")
    print(f"  Belief dims: {[b.shape[-1] for b in result['beliefs']]}")

    # Top-level beliefs should exist
    assert result['top_level_beliefs'].shape[-1] == 8
    print(f"  Top beliefs shape: {result['top_level_beliefs'].shape}")


def test_precision_attention_integration():
    """
    Test 6: AttentionAsPrecision should produce valid attention maps.

    Precision = 1/variance of prediction errors.
    Low-variance (reliable) channels get high precision -> high attention.
    """
    attn = AttentionAsPrecision(num_channels=16, tau=5.0, max_precision=10.0)

    # Steady reliable signal -> precision builds up
    for _ in range(30):
        signal = torch.ones(1, 16) * 0.5
        errors = torch.randn(1, 16) * 0.1  # Low-variance errors
        result = attn(signal, errors)

    attention = result['attention_map']
    precision = result['precision']

    # Attention should sum to 1 (softmax)
    assert abs(attention.sum().item() - 1.0) < 0.01, \
        f"Attention doesn't sum to 1: {attention.sum().item()}"

    # Precision should be high (low error variance)
    assert precision.mean().item() > 1.0, \
        f"Precision too low for reliable signal: {precision.mean().item():.4f}"

    print(f"  Mean precision: {precision.mean().item():.4f}")
    print(f"  Attention sum: {attention.sum().item():.4f}")
    print(f"  Attention range: [{attention.min().item():.4f}, {attention.max().item():.4f}]")


if __name__ == '__main__':
    print("=== Predictive Coding Validation ===\n")

    print("Test 1: Free energy decreases")
    test_free_energy_decreases()

    print("\nTest 2: Prediction errors decrease")
    test_prediction_errors_decrease()

    print("\nTest 3: Predictions reconstruct input")
    test_predictions_reconstruct_input()

    print("\nTest 4: Precision modulates learning rate")
    test_precision_modulates_learning()

    print("\nTest 5: Hierarchical message passing")
    test_hierarchy_message_passing()

    print("\nTest 6: Precision-attention integration")
    test_precision_attention_integration()

    print("\n[PASS] All predictive coding validation tests passed!")
