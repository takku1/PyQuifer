[build-system]
requires = ["setuptools>=61.0"]
build-backend = "setuptools.build_meta"

[project]
name = "pyquifer"
version = "0.1.0"
description = "A PyTorch library for oscillatory consciousness, embodied cognition, and ethics as physics."
readme = "README.md"
requires-python = ">=3.8"
authors = [
  { name = "takku1" },
]
classifiers = [
    "Development Status :: 3 - Alpha",
    "Intended Audience :: Science/Research",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.8",
    "Programming Language :: Python :: 3.9",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
    "Topic :: Scientific/Engineering :: Artificial Intelligence",
    "Topic :: Scientific/Engineering :: Physics",
    "Private :: Do Not Upload",
]
dependencies = [
    "torch>=2.0.0",
    "numpy",
    "pandas",
    "scikit-learn>=1.2",
]

[project.optional-dependencies]
docs = [
    "sphinx>=7.0.0",
    "sphinx-rtd-theme>=2.0.0",
    "sphinx-autodoc-typehints>=2.0.0",
    "myst-parser>=2.0.0",
]
dev = [
    "pytest>=7.0.0",
    "pytest-asyncio>=0.21.0",
    "pytest-timeout>=2.2.0",
    "matplotlib>=3.5.0",
]
compile = [
    "triton-windows>=3.6.0; sys_platform == 'win32'",
    "triton>=3.0.0; sys_platform == 'linux'",
]

[project.urls]
"Homepage" = "https://github.com/takku1/PyQuifer"

[tool.setuptools]
package-dir = {"" = "src"}

[tool.setuptools.packages.find]
where = ["src"]
include = ["pyquifer*"]

[tool.pytest.ini_options]
testpaths = ["tests"]
asyncio_mode = "auto"
norecursedirs = [
    "alphagenome",
    "bbeh",
    "beir",
    "BIG-bench",
    "bigcode-evaluation-harness",
    "bifurcation",
    "chessqa-benchmark",
    "code_contests",
    "dm_control",
    "EGG",
    "emergent_communication_at_scale",
    "FactBench",
    "FactReasoner",
    "FActScore",
    "formal-conjectures",
    "gpqa",
    "grade-school-math",
    "Gymnasium",
    "hanabi-learning-environment",
    "HarmBench",
    "helm",
    "human-eval",
    "jailbreakbench",
    "KuraNet",
    "lab",
    "lava",
    "lm-evaluation-harness",
    "lmms-eval",
    "MD-Bench",
    "meltingpot",
    "mlperf-inference",
    "mlperf-training",
    "MMMU",
    "mteb",
    "neurobench",
    "neurodiscoverybench",
    "nlb_tools",
    "open_spiel",
    "openai-evals",
    "opencompass",
    "osbenchmarks",
    "perception_test",
    "physics-IQ-benchmark",
    "Plaskett_puzzle",
    "PredBench",
    "SciMLBenchmarks.jl",
    "searchless_chess",
    "SWE-bench",
    "Torch2PC",
    "torchdiffeq",
    "torchdyn",
    "TOXIGEN",
    "TruthfulQA",
    "Video-MME",
    "VLMEvalKit",
]
